{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this second project I would like to focus on these objectives:\n",
    "\n",
    "1. Average Compensation by Country:\n",
    "\n",
    "    *   Calculate the average CompTotal or ConvertedCompYearly for each country.\n",
    "    *    Visualize the results using appropriate plots (e.g., bar charts or maps).\n",
    "\n",
    "2. Determine the average annual compensation for each developer role type from the survey data:\n",
    "    *   Calculate the averageConvertedCompYearly for Developer types from the DevType column.\n",
    "    *   Visualize the results using appropriate plots (e.g., bar charts or maps).\n",
    "\n",
    "3. Explore the relationship between years of professional coding experience (YearsCodePro) and annual compensation (ConvertedCompYearly) among developers:\n",
    "    *   Find the coefficient and correlation between Yearscodepro and ConvertedCompYearly columns\n",
    "    *   Visualize the results using appropriate plots (e.g., bar charts or maps).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # import necessary libraries for this project\n",
    "\n",
    "from pyspark.sql import SparkSession, functions as F, DataFrame\n",
    "from pyspark.sql.functions import col, when, avg\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Initialize Spark session with local file system configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CSV to DataFrame\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"file:///\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# This config is to ensure output can be readable and not overlapping\n",
    "spark.conf.set(\"spark.sql.debug.maxToStringFields\", \"100\") \n",
    " \n",
    "# Define the path to the CSV file\n",
    "csv_path = \"survey_results_public.csv\"\n",
    "# Read the CSV file into a DataFrame\n",
    "df = spark.read.csv(csv_path, header=True, inferSchema=True)\n",
    "\n",
    "# Count the number of rows and columns\n",
    "num_rows = df.count()\n",
    "num_columns = len(df.columns)\n",
    "print(f\"Number of rows: {num_rows}\")\n",
    "print(f\"Number of columns: {num_columns}\")\n",
    "\n",
    "#output : Number of rows: 89184 / Number of columns: 84"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp = df.select('CompTotal', 'ConvertedCompYearly', 'YearsCode', 'Country', 'DevType','YearsCodePro')\n",
    "df_comp.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark will not be able to interpret 'NA' as a missing value, which could lead to inaccurate results.\n",
    " Converting 'NA' to NULL ensures consistency in handling missing values. This is important for data quality and when performing transformations or analyses that rely on accurate handling of missing data.\n",
    "\n",
    "PySpark functions and methods are designed to handle NULL values natively, making it easier to work with the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def replace_NA_with_Null(df, columns):\n",
    "    for col in columns:\n",
    "        df = df.withColumn(col, F.when(F.col(col) == 'NA', None).otherwise(F.col(col)))\n",
    "    return df\n",
    "\n",
    "\n",
    "# Replace 'NA' with NULL\n",
    "columns_to_check = ['CompTotal', 'ConvertedCompYearly', 'YearsCode', 'Country', 'DevType', 'YearsCodePro']\n",
    "\n",
    "df_comp_cleaned = replace_NA_with_Null(df_comp, columns_to_check)\n",
    "\n",
    "df_comp_cleaned.show(5)\n",
    "\n",
    "df_comp_cleaned.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have cleaned the data, we can proceed to the transformation stage. We can cast the numbers columns to integer or float and make string literals into integer based values usefull for aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_years_code(df: DataFrame, columns: list) -> DataFrame:\n",
    "    for col in columns:\n",
    "        df = df.withColumn(\n",
    "            col,\n",
    "            F.when(F.col(col) == 'Less than 1 year', 0)\n",
    "                .when(F.col(col) == 'More than 50 years', 51)\n",
    "                .otherwise(F.col(col))\n",
    "        ).withColumn(col, F.col(col).cast('integer'))\n",
    "    return df\n",
    "\n",
    "# Columns to transform\n",
    "columns_to_transform = ['YearsCode', 'YearsCodePro']\n",
    "\n",
    "# Apply the transformation function to df_comp_cleaned\n",
    "df_comp_transformed = transform_years_code(df_comp_cleaned, columns_to_transform)\n",
    "\n",
    "# Print the schema to verify changes\n",
    "df_comp_transformed.printSchema()\n",
    "\n",
    "# Show the transformed DataFrame\n",
    "df_comp_transformed.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_to_int(df: DataFrame, columns: list) -> DataFrame:\n",
    "    for col in columns:\n",
    "        df = df.withColumn(col, F.col(col).cast('integer')\n",
    "        )\n",
    "    return df\n",
    "columns_to_cast = ['ConvertedCompYearly', 'CompTotal']\n",
    "\n",
    "df_comp_transformed = cast_to_int(df_comp_transformed, columns_to_cast)\n",
    "\n",
    "\n",
    "# Print the schema to verify changes\n",
    "df_comp_transformed.printSchema()\n",
    "\n",
    "# Show the transformed DataFrame\n",
    "df_comp_transformed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are renaming the Other(please specify) entry to Others for better readability. This column has users select from a list of developer roles and there was a field to enter a role if not present in the list.\n",
    "We will first check if there were other roles added which were not in the predefined list and then add them to the column, if not , we will consider them as roles in the Others category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "def update_devtype_and_add_unlisted(df: DataFrame, valid_dev_types: list) -> DataFrame:\n",
    "    # Define a new column where unlisted entries are preserved\n",
    "    df = df.withColumn(\n",
    "        'DevType',\n",
    "        F.when(F.col('DevType') == 'Other (please specify):', 'Others')\n",
    "        .otherwise(F.col('DevType'))\n",
    "    )\n",
    "    \n",
    "    # Get distinct entries in the DevType column\n",
    "    distinct_devtypes = df.select(F.col('DevType')).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "    \n",
    "    # Identify entries not in the valid list\n",
    "    other_devtypes = set(distinct_devtypes) - set(valid_dev_types) - {'Others'}\n",
    "    \n",
    "    # Print the other developer types that are not in the valid list\n",
    "    print(\"Other developer types not in the valid list:\", other_devtypes)\n",
    "    \n",
    "    # Map these entries to 'Others' if needed or keep them as is\n",
    "    # For this example, we assume that all unlisted entries are kept as is\n",
    "\n",
    "    # Return the DataFrame with updated DevType column\n",
    "    return df\n",
    "\n",
    "# List of valid developer types\n",
    "valid_dev_types = [\n",
    "    'Academic researcher',\n",
    "    'Cloud infrastructure engineer',\n",
    "    'Blockchain',\n",
    "    'Data or business analyst',\n",
    "    'Data scientist or machine learning specialist',\n",
    "    'Database administrator',\n",
    "    'Designer',\n",
    "    'Developer Advocate',\n",
    "    'Developer, back-end',\n",
    "    'Developer, desktop or enterprise applications',\n",
    "    'Developer, embedded applications or devices',\n",
    "    'Developer Experience',\n",
    "    'Developer, front-end',\n",
    "    'Developer, full-stack',\n",
    "    'Developer, game or graphics',\n",
    "    'Developer, mobile',\n",
    "    'Developer, QA or test',\n",
    "    'DevOps specialist',\n",
    "    'Educator',\n",
    "    'Engineer, data',\n",
    "    'Engineer, site reliability',\n",
    "    'Engineering manager',\n",
    "    'Hardware Engineer',\n",
    "    'Marketing or sales professional',\n",
    "    'Product manager',\n",
    "    'Project manager',\n",
    "    'Research & Development role',\n",
    "    'Scientist',\n",
    "    'Senior Executive (C-Suite, VP, etc.)',\n",
    "    'Student',\n",
    "    'System administrator',\n",
    "    'Security professional'\n",
    "]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to df_comp_transformed\n",
    "df_comp_transformed = update_devtype_and_add_unlisted(df_comp_transformed, valid_dev_types)\n",
    "\n",
    "# Show the updated DataFrame\n",
    "df_comp_transformed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Transformation stage is completed. Now proceeding to the visualisation step:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of users by Country\n",
    "user_count_by_country = df_comp_transformed.groupBy('Country').count().alias('UserCount')\n",
    "\n",
    "# Get top 10 countries by number of users\n",
    "top_10_countries_by_users = user_count_by_country.orderBy(col('count').desc()).limit(20)\n",
    "\n",
    "# Show the top 10 countries\n",
    "top_10_countries_by_users.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Convert top 10 countries to a list\n",
    "top_10_countries_list = [row['Country'] for row in top_10_countries_by_users.collect()]\n",
    "\n",
    "# Filter the original DataFrame for these top 10 countries\n",
    "df_top_10_countries = df_comp_transformed.filter(col('Country').isin(top_10_countries_list))\n",
    "\n",
    "# Calculate average ConvertedCompYearly by Country for top 10 countries\n",
    "avg_comp_top_10 = df_top_10_countries.groupBy('Country').agg(avg(col('ConvertedCompYearly')).alias('AverageCompYearly'))\n",
    "\n",
    "# Convert to Pandas DataFrame for visualization\n",
    "avg_comp_top_10_pd = avg_comp_top_10.toPandas()\n",
    "\n",
    "# Show the results\n",
    "avg_comp_top_10_pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the user count data with average compensation data\n",
    "top_10_countries_combined = top_10_countries_by_users.join(avg_comp_top_10, on='Country')\n",
    "\n",
    "# Convert to Pandas DataFrame for visualization\n",
    "top_10_countries_combined_pd = top_10_countries_combined.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot average compensation for top 10 countries with user counts\n",
    "plt.figure(figsize=(14, 8))\n",
    "bars = plt.barh(top_10_countries_combined_pd['Country'], top_10_countries_combined_pd['AverageCompYearly'], color='royalblue')\n",
    "\n",
    "# Add annotations for average compensation and number of users\n",
    "for bar, (index, row) in zip(bars, top_10_countries_combined_pd.iterrows()):\n",
    "    width = bar.get_width()\n",
    "    avg_comp = row['AverageCompYearly']\n",
    "    count = row['count']\n",
    "    \n",
    "    plt.text(\n",
    "        width + 10000,  # X coordinate: slightly to the right of the end of the bar\n",
    "        bar.get_y() + bar.get_height() / 2,\n",
    "        f'${avg_comp:,.0f}\\nUsers: {count}',\n",
    "        va='center',\n",
    "        ha='left',\n",
    "        color='black',\n",
    "        fontsize=10,\n",
    "        bbox=dict(facecolor='white', edgecolor='black', boxstyle='round,pad=0.5')  # Background box for readability\n",
    "    )\n",
    "\n",
    "plt.xlabel('Average Compensation (USD)')\n",
    "plt.ylabel('Country')\n",
    "plt.title('Top 10 Countries by Number of Users and Their Average Compensation (USD)')\n",
    "plt.gca().invert_yaxis()  # Highest values at the top\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average ConvertedCompYearly by DevType\n",
    "avg_converted_comp_by_devtype = df_comp_transformed.groupBy(\"DevType\").agg(\n",
    "    avg(\"ConvertedCompYearly\").alias(\"Avg_ConvertedCompYearly\")\n",
    ")\n",
    "\n",
    "# Show the result\n",
    "avg_converted_comp_by_devtype.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows with None values in DevType or Avg_ConvertedCompYearly\n",
    "avg_converted_comp_by_devtype_filtered = avg_converted_comp_by_devtype.filter(\n",
    "    col('DevType').isNotNull() & col('Avg_ConvertedCompYearly').isNotNull()\n",
    ")\n",
    "\n",
    "# Convert the filtered result to a Pandas DataFrame for easier plotting\n",
    "avg_converted_comp_by_devtype_pd = avg_converted_comp_by_devtype_filtered.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.barh(avg_converted_comp_by_devtype_pd['DevType'], avg_converted_comp_by_devtype_pd['Avg_ConvertedCompYearly'], color='royalblue')\n",
    "\n",
    "# Add annotations for average compensation\n",
    "for index, value in enumerate(avg_converted_comp_by_devtype_pd['Avg_ConvertedCompYearly']):\n",
    "    plt.text(value, index, f'${value:,.0f}', va='center', ha='left', color='black', fontsize=10)\n",
    "\n",
    "plt.xlabel('Average Compensation (USD)')\n",
    "plt.ylabel('Developer Type')\n",
    "plt.title('Average Compensation by Developer Type')\n",
    "plt.gca().invert_yaxis()  # Highest values at the top\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Calculate correlation between YearsCodePro and ConvertedCompYearly\n",
    "correlation_years_code_pro_converted_comp_yearly = df_comp_transformed.corr('YearsCodePro', 'ConvertedCompYearly')\n",
    "\n",
    "\n",
    "print(f\"Correlation between YearsCodePro and ConvertedCompYearly: {correlation_years_code_pro_converted_comp_yearly:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Create bins for YearsCode if not already done\n",
    "df_comp_transformed_cleaned = df_comp_transformed.withColumn(\n",
    "    'YearsCodeCategory',\n",
    "    when(col('YearsCode') <= 18, 'Under 18 years old')\n",
    "    .when((col('YearsCode') > 18) & (col('YearsCode') <= 24), '18-24 years old')\n",
    "    .when((col('YearsCode') > 24) & (col('YearsCode') <= 34), '25-34 years old')\n",
    "    .when((col('YearsCode') > 34) & (col('YearsCode') <= 44), '35-44 years old')\n",
    "    .when((col('YearsCode') > 44) & (col('YearsCode') <= 54), '45-54 years old')\n",
    "    .when((col('YearsCode') > 54) & (col('YearsCode') <= 64), '55-64 years old')\n",
    "    .otherwise('65 years or older')\n",
    ")\n",
    "\n",
    "# Calculate average compensation by binned years of experience\n",
    "avg_compensation_by_years_code = df_comp_transformed_cleaned.groupBy('YearsCodeCategory').agg(\n",
    "    {'ConvertedCompYearly': 'avg'}\n",
    ").withColumnRenamed('avg(ConvertedCompYearly)', 'AverageCompYearly')\n",
    "\n",
    "# Convert to Pandas DataFrame for plotting\n",
    "df_avg_comp_by_years_code = avg_compensation_by_years_code.toPandas()\n",
    "\n",
    "# Ensure 'YearsCodeCategory' is categorical and ordered\n",
    "df_avg_comp_by_years_code['YearsCodeCategory'] = pd.Categorical(\n",
    "    df_avg_comp_by_years_code['YearsCodeCategory'],\n",
    "    categories=['Under 18 years old', '18-24 years old', '25-34 years old', \n",
    "        '35-44 years old', '45-54 years old', '55-64 years old', \n",
    "        '65 years or older'],\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# Sort by the categorical order\n",
    "df_avg_comp_by_years_code = df_avg_comp_by_years_code.sort_values('YearsCodeCategory')\n",
    "\n",
    "# Plotting the line plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_avg_comp_by_years_code['YearsCodeCategory'], df_avg_comp_by_years_code['AverageCompYearly'], marker='o', color='teal')\n",
    "plt.title('Average Compensation by Years of Coding Experience')\n",
    "plt.xlabel('Years of Coding Experience')\n",
    "plt.ylabel('Average Converted Annual Compensation (USD)')\n",
    "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
